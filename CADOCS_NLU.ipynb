{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfcan/CADOCS_NLU_Model/blob/feat%2Fml%2FNLP-pipeline/CADOCS_NLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "QQYIffUvHTq6"
      },
      "outputs": [],
      "source": [
        "# Installation of the libraries which we'll use: HuggingFace Transformers, Datasets and Evaluate\n",
        "!pip install -q transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "akHy9lVIL2Me"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "from tqdm import trange\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "i9NBbZN8MbkM"
      },
      "outputs": [],
      "source": [
        "root = \"https://raw.githubusercontent.com/alfcan/CADOCS_NLU_Model/dev/augmented_dataset.csv\"\n",
        "\n",
        "df = pd.read_csv(root, sep = ';')\n",
        "label_mapping = {'get_smells': 0, 'get_smells_date': 1, 'report': 2, 'info': 3}\n",
        "df['intent'] = df['intent'].map(label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "VnkVGdBsNYO9"
      },
      "outputs": [],
      "source": [
        "request = df.request.values\n",
        "intent = df.intent.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "3EZDo4rNN6Yo"
      },
      "outputs": [],
      "source": [
        "# Divide the dataset into train and test sets\n",
        "# Test set is divided into a test set and a validation set\n",
        "test_ratio = 0.3\n",
        "val_ratio = 0.33\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Indices of the train set and temp set (validation + test sets) splits stratified by labels\n",
        "train_request, temp_request, train_intent, temp_intent = train_test_split(\n",
        "    request,\n",
        "    intent,\n",
        "    test_size = test_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = intent)\n",
        "\n",
        "# We divide then temp_idx in test_idx (test set) and val_idx ( validation set)\n",
        "test_request, val_request, test_intent, val_intent = train_test_split(\n",
        "    temp_request,\n",
        "    temp_intent,\n",
        "    test_size = val_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = temp_intent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "h9hn4_DNQiKG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "b427030b-42bc-4c23-d7c9-1e71a18826c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 137
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert NumPy array to a Pandas Series object\n",
        "train_intent_series = pd.Series(train_intent)\n",
        "\n",
        "# Creates a bar graph for counting the occurrences of each unique value\n",
        "train_intent_series.value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "mOV1xbfeX8oI"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Tokenizer of BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    do_lower_case = True\n",
        ")\n",
        "'''\n",
        "\n",
        "# Tokenizer of RoBERTa\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\n",
        "    'roberta-base',\n",
        "    do_lower_case = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "9u87jqpKYBj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0647b3e3-000d-47c9-ef1b-0b2d1b8d30f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Tokenize each set splitted\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 32,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "def preprocessing_batch(data_set):\n",
        "    token_id = []\n",
        "    attention_masks = []\n",
        "    for sample in data_set:\n",
        "      encoding_dict = preprocessing(sample, tokenizer)\n",
        "      token_id.append(encoding_dict['input_ids'])\n",
        "      attention_masks.append(encoding_dict['attention_mask'])\n",
        "    token_id = torch.cat(token_id, dim = 0)\n",
        "    attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "    return token_id,attention_masks\n",
        "\n",
        "train_token_id,train_attention_masks = preprocessing_batch(train_request)\n",
        "test_token_id,test_attention_masks = preprocessing_batch(test_request)\n",
        "val_token_id,val_attention_masks = preprocessing_batch(val_request)\n",
        "\n",
        "train_intent = torch.tensor(train_intent)\n",
        "test_intent = torch.tensor(test_intent)\n",
        "val_intent = torch.tensor(val_intent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "S4wYNjHJfspF"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "# Train, validation and test sets\n",
        "train_set = TensorDataset(train_token_id,\n",
        "                          train_attention_masks,\n",
        "                          train_intent)\n",
        "\n",
        "val_set = TensorDataset(val_token_id,\n",
        "                        val_attention_masks,\n",
        "                        val_intent)\n",
        "\n",
        "test_set = TensorDataset(test_token_id,\n",
        "                        test_attention_masks,\n",
        "                        test_intent)\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            test_set,\n",
        "            sampler = SequentialSampler(test_set),\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "IZUmYzo5j4q6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0119b21f-014f-4801-8b01-375ec5243590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "'''\n",
        "# Load the BertForSequenceClassification model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels = 4,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the RobertaForSequenceClassification model\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    'roberta-base',\n",
        "    num_labels = 4,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr = 3e-5,\n",
        "                              eps = 1e-10\n",
        "                              )\n",
        "\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the metrics\n",
        "import evaluate\n",
        "\n",
        "accuracy_metric = evaluate.load('accuracy')\n",
        "precision_metric = evaluate.load('precision')\n",
        "recall_metric = evaluate.load('recall')\n",
        "mcc_metric = evaluate.load('matthews_correlation')\n",
        "\n",
        "# Compute of metrics\n",
        "def compute_metrics(logits, label_ids):\n",
        "  preds = np.argmax(logits, axis=1)\n",
        "  val_accuracy = accuracy_metric.compute(predictions=preds, references=label_ids)['accuracy']\n",
        "  val_precision = precision_metric.compute(predictions=preds, references=label_ids, average='macro')['precision']\n",
        "  val_recall = recall_metric.compute(predictions=preds, references=label_ids, average='macro')['recall']\n",
        "  val_mcc = mcc_metric.compute(predictions=preds, references=label_ids)['matthews_correlation']\n",
        "\n",
        "  return val_accuracy, val_precision, val_recall, val_mcc"
      ],
      "metadata": {
        "id": "N6LixxViPfab"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "ykzG9HYVkAHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46822e0c-7d52-4e2e-a5c6-095f6dfef40e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  33%|███▎      | 1/3 [00:03<00:07,  3.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.8601\n",
            "\t - Validation Accuracy: 1.0000\n",
            "\t - Validation Precision: 1.0000\n",
            "\t - Validation Recall: 1.0000\n",
            "\t - Validation MCC: 1.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [00:09<00:04,  4.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.0625\n",
            "\t - Validation Accuracy: 1.0000\n",
            "\t - Validation Precision: 1.0000\n",
            "\t - Validation Recall: 1.0000\n",
            "\t - Validation MCC: 1.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [00:13<00:00,  4.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.0115\n",
            "\t - Validation Accuracy: 1.0000\n",
            "\t - Validation Precision: 1.0000\n",
            "\t - Validation Recall: 1.0000\n",
            "\t - Validation MCC: 1.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "\n",
        "    # ========== Training ==========\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        #reset gradient value for the new epoch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids,\n",
        "                             token_type_ids = None,\n",
        "                             attention_mask = b_input_mask,\n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids,\n",
        "                              token_type_ids = None,\n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    val_accuracy, val_precision, val_recall, val_mcc = compute_metrics(logits, label_ids)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(val_accuracy))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(val_precision))\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(val_recall))\n",
        "    print('\\t - Validation MCC: {:.4f}\\n'.format(val_mcc))\n",
        "\n",
        "# Save the trained model to a file\n",
        "PATH = './bert.pth'\n",
        "torch.save(model, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "YcE71tSuYKLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a26fe1-1eec-43d1-cc72-9be1550b9808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.0115\n",
            "\t - Validation Accuracy: 1.0000\n",
            "\t - Validation Precision: 1.0000\n",
            "\t - Validation Recall: 1.0000\n",
            "\t - Validation MCC: 1.0000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load the model saved\n",
        "PATH = './bert.pth'\n",
        "model = torch.load(PATH)\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    eval_output = model(b_input_ids,\n",
        "                        token_type_ids = None,\n",
        "                        attention_mask = b_input_mask)\n",
        "    logits = eval_output.logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "# Calculate validation metrics\n",
        "val_accuracy, val_precision, val_recall, val_mcc = compute_metrics(logits, label_ids)\n",
        "\n",
        "print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "print('\\t - Validation Accuracy: {:.4f}'.format(val_accuracy))\n",
        "print('\\t - Validation Precision: {:.4f}'.format(val_precision))\n",
        "print('\\t - Validation Recall: {:.4f}'.format(val_recall))\n",
        "print('\\t - Validation MCC: {:.4f}\\n'.format(val_mcc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input testing\n",
        "requests_smells = [\n",
        "                    \"Hello CADOCS, which community smells have this project LINK?\",\n",
        "                    \"Hi CADOCS, give me the community smells on this project LINK\",\n",
        "                    \"CADOCS, community smells on the project LINK?\",\n",
        "                    \"Give me code smells on repository LINK\"\n",
        "                  ]\n",
        "\n",
        "requests_smells_date = [\n",
        "                        \"Hello CADOCS, which community smells have this project LINK after 18/05/2022?\",\n",
        "                        \"Hi CADOCS, give me the community smells after DATE on this project LINK\",\n",
        "                        \"CADOCS, community smells starting from 10/06/2022 on the project LINK?\",\n",
        "                        \"Give me code smells on LINK after 06/02/2020\"\n",
        "                       ]\n",
        "\n",
        "requests_report = [\n",
        "                    \"Hello CADOCS, can you give me the report of the last execution?\",\n",
        "                    \"CADOCS, give me the report of the last execution\",\n",
        "                    \"Hi CADOCS, give me the last execution\",\n",
        "                    \"Give me the report of the last execution\"\n",
        "                  ]\n",
        "\n",
        "requests_info =   [\n",
        "                    \"CADOCS what are community smells?\",\n",
        "                    \"What can you do?\",\n",
        "                    \"Which types of community smells can you return?\",\n",
        "                    \"Hi CADOCS, what can you do?\",\n",
        "                    \"Talk me about community smells\"\n",
        "                  ]\n",
        "\n",
        "requests_arrays = [requests_smells, requests_smells_date, requests_report, requests_info]\n",
        "array_names = ['requests_smells', 'requests_smells_date', 'requests_report', 'requests_info']\n",
        "\n",
        "for requests, array_name in zip(requests_arrays, array_names):\n",
        "  print(\"\\nArray:\", array_name)\n",
        "\n",
        "  for request in requests:\n",
        "    request_ids = tokenizer.encode_plus(\n",
        "                            request,\n",
        "                            add_special_tokens = True,\n",
        "                            max_length = 32,\n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,\n",
        "                            return_tensors = 'pt'\n",
        "                      )\n",
        "\n",
        "    input_ids = torch.clone(request_ids['input_ids']).detach().to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids)\n",
        "\n",
        "    predictions = output.logits.detach().cpu().numpy()\n",
        "    predicted_class = np.argmax(predictions, axis = 1).flatten()\n",
        "\n",
        "    for label, index in label_mapping.items():\n",
        "        if index == predicted_class:\n",
        "            predicted_label = label\n",
        "            break\n",
        "\n",
        "    print(\"Predicted class: \" + predicted_label + \" --- \" + request)"
      ],
      "metadata": {
        "id": "FDUpb5_JMOCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75dab9df-27ac-434a-dc09-0070ae805381"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Array: requests_smells\n",
            "Predicted class: get_smells --- Hello CADOCS, which community smells have this project LINK?\n",
            "Predicted class: get_smells --- Hi CADOCS, give me the community smells on this project LINK\n",
            "Predicted class: get_smells --- CADOCS, community smells on the project LINK?\n",
            "Predicted class: get_smells --- Give me code smells on repository LINK\n",
            "\n",
            "Array: requests_smells_date\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: get_smells_date --- Hello CADOCS, which community smells have this project LINK after 18/05/2022?\n",
            "Predicted class: get_smells_date --- Hi CADOCS, give me the community smells after DATE on this project LINK\n",
            "Predicted class: get_smells_date --- CADOCS, community smells starting from 10/06/2022 on the project LINK?\n",
            "Predicted class: get_smells_date --- Give me code smells on LINK after 06/02/2020\n",
            "\n",
            "Array: requests_report\n",
            "Predicted class: report --- Hello CADOCS, can you give me the report of the last execution?\n",
            "Predicted class: report --- CADOCS, give me the report of the last execution\n",
            "Predicted class: report --- Hi CADOCS, give me the last execution\n",
            "Predicted class: get_smells --- Give me the report of the last execution\n",
            "\n",
            "Array: requests_info\n",
            "Predicted class: info --- CADOCS what are community smells?\n",
            "Predicted class: info --- What can you do?\n",
            "Predicted class: get_smells --- Which types of community smells can you return?\n",
            "Predicted class: info --- Hi CADOCS, what can you do?\n",
            "Predicted class: get_smells --- Talk me about community smells\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}